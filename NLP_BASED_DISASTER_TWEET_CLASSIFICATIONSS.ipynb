{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adnanbaqi/nlp_based_disaster_tweets/blob/master/NLP_BASED_DISASTER_TWEET_CLASSIFICATIONSS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here begins the model development"
      ],
      "metadata": {
        "id": "Ohidu9sn7cLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install flash-attn\n",
        "!pip install torch pandas numpy scikit-learn tqdm\n"
      ],
      "metadata": {
        "id": "i0lpT1B8L1qv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccad0fef-cd68-481e-c19b-c3df37bdca11",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-e_k3mmzz\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-e_k3mmzz\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 24c91f095fec4d90fa6901ef17146b4f4c21d0a3\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0.dev0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (2024.12.14)\n",
            "Requirement already satisfied: flash-attn in /usr/local/lib/python3.10/dist-packages (2.7.2.post1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.5.1+cu121)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ModernBERT + GloVe Hybrid Model\n",
        "class ModernBertGloveHybrid(nn.Module):\n",
        "    def __init__(self, model_name='answerdotai/ModernBERT-base', freeze_bert=False):\n",
        "        super(ModernBertGloveHybrid, self).__init__()\n",
        "\n",
        "        # ModernBERT components\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.bert_hidden_size = self.bert.config.hidden_size\n",
        "\n",
        "        # GloVe components\n",
        "        self.glove_dim = 100\n",
        "        self.glove_embeddings = {}\n",
        "\n",
        "        # Fusion and classification layers\n",
        "        self.fusion = nn.Linear(self.bert_hidden_size + self.glove_dim, self.bert_hidden_size)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.bert_hidden_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, 2)\n",
        "        )\n",
        "\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def initialize_glove(self, glove_path):\n",
        "        \"\"\"Initialize GloVe embeddings\"\"\"\n",
        "        print(\"Loading GloVe embeddings...\")\n",
        "        if glove_path:\n",
        "            try:\n",
        "                with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "                    for line in f:\n",
        "                        values = line.split()\n",
        "                        word = values[0]\n",
        "                        vector = np.asarray(values[1:], dtype='float32')\n",
        "                        self.glove_embeddings[word] = vector\n",
        "\n",
        "                self.glove_embeddings['[PAD]'] = np.zeros(100)\n",
        "                self.glove_embeddings['[UNK]'] = np.mean([vec for vec in self.glove_embeddings.values()], axis=0)\n",
        "                print(\"GloVe embeddings loaded successfully!\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading GloVe embeddings: {e}\")\n",
        "                raise FileNotFoundError(\"GloVe embeddings file not found, please provide a valid file path.\")\n",
        "        else:\n",
        "            self.glove_embeddings = {'[PAD]': np.zeros(100), '[UNK]': np.random.randn(100)}\n",
        "            print(\"Using random GloVe embeddings.\")\n",
        "\n",
        "    def get_glove_embedding(self, text):\n",
        "        \"\"\"Get GloVe embedding for a text\"\"\"\n",
        "        words = text.lower().split()\n",
        "        word_embeddings = []\n",
        "        for word in words:\n",
        "            if word in self.glove_embeddings:\n",
        "                word_embeddings.append(self.glove_embeddings[word])\n",
        "            else:\n",
        "                word_embeddings.append(self.glove_embeddings['[UNK]'])\n",
        "\n",
        "        if not word_embeddings:\n",
        "            return torch.zeros(self.glove_dim, dtype=torch.float32)\n",
        "\n",
        "        return torch.tensor(np.mean(word_embeddings, axis=0), dtype=torch.float32)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, texts):\n",
        "        # Get ModernBERT outputs\n",
        "        bert_outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        bert_pooled = bert_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Process GloVe embeddings and ensure float32\n",
        "        glove_embeddings = [self.get_glove_embedding(text) for text in texts]\n",
        "        glove_embeddings = torch.stack(glove_embeddings).to(bert_pooled.device).to(bert_pooled.dtype)\n",
        "\n",
        "        # Feature fusion\n",
        "        combined_features = torch.cat([bert_pooled, glove_embeddings], dim=1)\n",
        "        fused_features = self.fusion(combined_features)\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(fused_features)\n",
        "        return logits\n",
        "\n",
        "# Dataset Class\n",
        "class DisasterTweetDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Main Classifier\n",
        "class DisasterTweetClassifier:\n",
        "    def __init__(self, model_name='answerdotai/ModernBERT-base', max_length=128):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = ModernBertGloveHybrid(model_name).to(self.device)\n",
        "        self.max_length = max_length\n",
        "\n",
        "        try:\n",
        "            self.model.initialize_glove('glove.6B.100d.txt')\n",
        "        except FileNotFoundError:\n",
        "            print(\"GloVe embeddings file not found. Using random embeddings...\")\n",
        "            self.model.initialize_glove(None)\n",
        "\n",
        "    def train(self, train_loader, val_loader, epochs=3, lr=2e-5):\n",
        "        self.model.train()\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        best_val_acc = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            train_loss = 0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} - Training'):\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                labels = batch['label'].to(self.device)\n",
        "                texts = batch['text']\n",
        "\n",
        "                outputs = self.model(input_ids, attention_mask, texts)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                train_total += labels.size(0)\n",
        "                train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            # Validation phase\n",
        "            self.model.eval()\n",
        "            val_loss = 0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} - Validation'):\n",
        "                    input_ids = batch['input_ids'].to(self.device)\n",
        "                    attention_mask = batch['attention_mask'].to(self.device)\n",
        "                    labels = batch['label'].to(self.device)\n",
        "                    texts = batch['text']\n",
        "\n",
        "                    outputs = self.model(input_ids, attention_mask, texts)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = outputs.max(1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            train_acc = 100. * train_correct / train_total\n",
        "            val_acc = 100. * val_correct / val_total\n",
        "\n",
        "            print(f'\\nEpoch {epoch+1}:')\n",
        "            print(f'Training Loss: {train_loss/len(train_loader):.4f}, Accuracy: {train_acc:.2f}%')\n",
        "            print(f'Validation Loss: {val_loss/len(val_loader):.4f}, Accuracy: {val_acc:.2f}%')\n",
        "\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                torch.save(self.model.state_dict(), 'best_model.pt')\n",
        "                print(f'New best model saved with validation accuracy: {val_acc:.2f}%')\n",
        "\n",
        "    def predict(self, test_loader):\n",
        "        self.model.eval()\n",
        "        predictions = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_loader, desc='Generating predictions'):\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                texts = batch['text']\n",
        "\n",
        "                outputs = self.model(input_ids, attention_mask, texts)\n",
        "                predictions.extend(outputs.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "        return predictions\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean and preprocess tweet text\"\"\"\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'@\\w+', '', text)\n",
        "        text = re.sub(r'#', '', text)\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        return ' '.join(text.split())\n",
        "    return ''\n",
        "\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    print(\"Loading local dataset files...\")\n",
        "    try:\n",
        "        train_df = pd.read_csv('nlp_dataset/train.csv')\n",
        "        test_df = pd.read_csv('nlp_dataset/test.csv')\n",
        "    except FileNotFoundError:\n",
        "        print(\"Dataset files not found. Please check the paths.\")\n",
        "        return\n",
        "\n",
        "    print(\"Preprocessing texts...\")\n",
        "    train_df['text'] = train_df['text'].apply(preprocess_text)\n",
        "    test_df['text'] = test_df['text'].apply(preprocess_text)\n",
        "\n",
        "    # Split training data\n",
        "    print(\"Preparing training and validation splits...\")\n",
        "    if 'text' not in train_df or 'target' not in train_df:\n",
        "        raise ValueError(\"Dataset missing required columns: 'text' or 'target'.\")\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        train_df['text'].values,\n",
        "        train_df['target'].values,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=train_df['target']\n",
        "    )\n",
        "\n",
        "    # Initialize classifier\n",
        "    print(\"Initializing ModernBERT classifier...\")\n",
        "    classifier = DisasterTweetClassifier()\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = DisasterTweetDataset(X_train, y_train, classifier.tokenizer)\n",
        "    val_dataset = DisasterTweetDataset(X_val, y_val, classifier.tokenizer)\n",
        "    test_dataset = DisasterTweetDataset(\n",
        "        test_df['text'].values,\n",
        "        np.zeros(len(test_df)),\n",
        "        classifier.tokenizer\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\nStarting model training...\")\n",
        "    classifier.train(train_loader, val_loader)\n",
        "\n",
        "    # Generate predictions\n",
        "    print(\"\\nGenerating predictions...\")\n",
        "    predictions = classifier.predict(test_loader)\n",
        "\n",
        "    # Save predictions\n",
        "    submission_df = pd.DataFrame({\n",
        "        'id': test_df['id'],\n",
        "        'target': predictions\n",
        "    })\n",
        "    submission_df.to_csv('submission.csv', index=False)\n",
        "    print(\"Predictions saved to: submission.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fo4Ba4OhWOys",
        "outputId": "65a7a675-7504-4cd3-967b-5ff7d12d6df8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading local dataset files...\n",
            "Preprocessing texts...\n",
            "Preparing training and validation splits...\n",
            "Initializing ModernBERT classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe embeddings...\n",
            "GloVe embeddings loaded successfully!\n",
            "\n",
            "Starting model training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:   1%|          | 1/191 [01:19<4:11:03, 79.28s/it]"
          ]
        }
      ]
    }
  ]
}